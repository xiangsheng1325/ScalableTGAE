{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os.path\n",
    "import pickle\n",
    "from curses import raw\n",
    "from dgl.dataloading import MultiLayerFullNeighborSampler\n",
    "from dgl.dataloading import DataLoader\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import numpy as np\n",
    "from data_utils import *\n",
    "from eval_utils import *\n",
    "from model_utils import *\n",
    "from models import *\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from eval_utils import compute_graph_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FromTemporalGraphToSparseAdj(filename='./data/DBLP/edgelist_new.txt',\n",
    "                                 save_path='./data/DBLP/dgl_graph.bin'):\n",
    "    \n",
    "    DBLP = np.loadtxt(filename, dtype=int)\n",
    "    TemporalGraph = {\n",
    "        \"src\": DBLP[:, 0],\n",
    "        \"dst\": DBLP[:, 1],\n",
    "        \"timestamp\": DBLP[:, 2]\n",
    "    }\n",
    "    node_num = len(np.unique(np.concatenate([TemporalGraph[\"src\"], TemporalGraph[\"dst\"]])))\n",
    "    time_unique = np.unique(TemporalGraph[\"timestamp\"])\n",
    "    temporal_src = TemporalGraph[\"src\"] + node_num * TemporalGraph[\"timestamp\"]\n",
    "    temporal_dst = TemporalGraph[\"dst\"] + node_num * TemporalGraph[\"timestamp\"]\n",
    "    inner_src = np.array(range(time_unique.min() * node_num, time_unique.max() * node_num))\n",
    "    inner_dst = np.array(range((time_unique.min() + 1) * node_num, (time_unique.max() + 1) * node_num))\n",
    "    self_src = np.array(range(time_unique.min() * node_num, (time_unique.max() + 1) * node_num))\n",
    "    self_dst = np.array(range(time_unique.min() * node_num, (time_unique.max() + 1) * node_num))\n",
    "    temporal_edges = (np.concatenate([temporal_src, inner_src, self_src]),\n",
    "                      np.concatenate([temporal_dst, inner_dst, self_dst]))\n",
    "    dglg = dgl.graph(temporal_edges)\n",
    "    dgl.data.utils.save_graphs(save_path, [dglg])\n",
    "    target_src = TemporalGraph[\"src\"] + node_num * TemporalGraph[\"timestamp\"]\n",
    "    target_dst = TemporalGraph[\"dst\"]\n",
    "    train_nids = np.unique(target_src)\n",
    "    return sp.coo_matrix((np.ones(len(target_src)), (target_src, target_dst)),\n",
    "                         shape=(node_num * (time_unique.max() - time_unique.min() + 1), node_num)), train_nids\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "EDGE_OVERLAP_LIMIT = {\n",
    "    'CORA-ML' : 0.9,\n",
    "}\n",
    "MAX_STEPS = 400\n",
    "\n",
    "def random_seed(seed=2024):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temporal_graph_statistics(A_T):\n",
    "    seq_len = A_T.shape[0]//A_T.shape[1]\n",
    "    num_nodes = A_T.shape[1]\n",
    "    A_seq = [A_T[i*num_nodes:(i+1)*num_nodes, :] for i in range(seq_len)]\n",
    "    stats = []\n",
    "    for i in range(seq_len):\n",
    "        stats.append(compute_graph_statistics(A_seq[i]))\n",
    "        \n",
    "    return stats\n",
    "\n",
    "def cal_avg_median_stats(adj, gen_mat):\n",
    "    stats_real = compute_temporal_graph_statistics(adj)\n",
    "    stats_gen_mat = compute_temporal_graph_statistics(gen_mat)\n",
    "    \n",
    "    f_avg = {}\n",
    "    f_med = {}\n",
    "    for key in stats_real[0].keys():\n",
    "        stats_real_values = np.array([stat[key] for stat in stats_real])\n",
    "        stats_gen_mat_values = np.array([stat[key] for stat in stats_gen_mat])\n",
    "        if 0 in stats_real_values:\n",
    "            # vals = abs((stats_real_values - stats_gen_mat_values))\n",
    "            pass\n",
    "        else:\n",
    "            vals = abs((stats_real_values - stats_gen_mat_values) / stats_real_values) \n",
    "            f_avg[key] = np.mean(vals)\n",
    "            \n",
    "            vals = vals[vals != 0]\n",
    "            f_med[key] = np.median(vals)\n",
    "    return f_avg, f_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseArguments(object):\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda:0'\n",
    "        self.n_layers = 1\n",
    "        self.H = 128\n",
    "        self.n_heads = 4\n",
    "        self.batch_size = 128\n",
    "        self.g_type = 'temporal'\n",
    "        self.lr = 4e-3\n",
    "        self.weight_decay = 1e-4\n",
    "        self.max_epochs = 500\n",
    "        self.graphic_mode = 'overlap'\n",
    "        self.criterion = 'eo'\n",
    "        self.eo_limit = 0.99\n",
    "        self.seed = 2024\n",
    "\n",
    "args = ParseArguments()\n",
    "if args.seed is not None:\n",
    "    random_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouzicai/GraphGeneration/ScalableTGAE/models.py:163: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1716905969073/work/torch/csrc/utils/tensor_new.cpp:621.)\n",
      "  sp_tensor = torch.sparse.FloatTensor(torch.LongTensor(np.stack([row, col])),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, overall loss: 7.2287492718\n",
      "Epoch: 002, overall loss: 5.8656312669\n",
      "Epoch: 003, overall loss: 4.6255952372\n",
      "Epoch: 004, overall loss: 3.4060272749\n",
      "Epoch: 005, overall loss: 2.6113304258\n",
      "Epoch: 006, overall loss: 2.4191749724\n",
      "Epoch: 007, overall loss: 2.5131967581\n",
      "Epoch: 008, overall loss: 2.5373709631\n",
      "Epoch: 009, overall loss: 2.4649710346\n",
      "Epoch: 010, overall loss: 2.3510799081\n",
      "Epoch: 011, overall loss: 2.2384969063\n",
      "Epoch: 012, overall loss: 2.1600494348\n",
      "Epoch: 013, overall loss: 2.1015901757\n",
      "Epoch: 014, overall loss: 2.0765011330\n",
      "Epoch: 015, overall loss: 2.0533577531\n",
      "Epoch: 016, overall loss: 2.0566752219\n",
      "Epoch: 017, overall loss: 2.0593846351\n",
      "Epoch: 018, overall loss: 2.0787205029\n",
      "Epoch: 019, overall loss: 2.0891453390\n",
      "Epoch: 020, overall loss: 2.0942802976\n",
      "Epoch: 021, overall loss: 2.0781964466\n",
      "Epoch: 022, overall loss: 2.0843616550\n",
      "Epoch: 023, overall loss: 2.0683588710\n",
      "Epoch: 024, overall loss: 2.0567217512\n",
      "Epoch: 025, overall loss: 2.0390666590\n",
      "Epoch: 026, overall loss: 2.0289331511\n",
      "Epoch: 027, overall loss: 2.0170061437\n",
      "Epoch: 028, overall loss: 2.0102546942\n",
      "Epoch: 029, overall loss: 2.0083291214\n",
      "Epoch: 030, overall loss: 2.0050972164\n",
      "Epoch: 031, overall loss: 2.0123004266\n",
      "Epoch: 032, overall loss: 2.0122233034\n",
      "Epoch: 033, overall loss: 2.0232547455\n",
      "Epoch: 034, overall loss: 2.0212582614\n",
      "Epoch: 035, overall loss: 2.0159493911\n",
      "Epoch: 036, overall loss: 2.0221622376\n",
      "Epoch: 037, overall loss: 2.0160695439\n",
      "Epoch: 038, overall loss: 2.0085957051\n",
      "Epoch: 039, overall loss: 2.0039781829\n",
      "Epoch: 040, overall loss: 2.0014765233\n",
      "Epoch: 041, overall loss: 1.9945228422\n",
      "Epoch: 042, overall loss: 1.9952162725\n",
      "Epoch: 043, overall loss: 1.9923976791\n",
      "Epoch: 044, overall loss: 1.9901135214\n",
      "Epoch: 045, overall loss: 1.9909030310\n",
      "Epoch: 046, overall loss: 1.9850324239\n",
      "Epoch: 047, overall loss: 1.9903781421\n",
      "Epoch: 048, overall loss: 1.9874125257\n",
      "Epoch: 049, overall loss: 1.9857415198\n",
      "Epoch: 050, overall loss: 1.9847616587\n",
      "Epoch: 050, Generating Step: 003\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouzicai/anaconda3/envs/gnn/lib/python3.11/site-packages/scipy/sparse/_index.py:151: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil and dok are more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "/home/zhouzicai/anaconda3/envs/gnn/lib/python3.11/site-packages/scipy/sparse/_index.py:142: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil and dok are more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Edge Overlap: 0.923273\n",
      "\n",
      "\n",
      "================================================================================\n",
      "f_avg(d_max): 0.141944\n",
      "f_med(d_max): 0.153846\n",
      "f_avg(d_mean): 0.065406\n",
      "f_med(d_mean): 0.058577\n",
      "f_avg(LCC): 0.087230\n",
      "f_med(LCC): 0.063218\n",
      "f_avg(wedge_count): 0.143725\n",
      "f_med(wedge_count): 0.123062\n",
      "f_avg(claw_count): 0.218998\n",
      "f_med(claw_count): 0.212979\n",
      "f_avg(power_law_exp): 0.041030\n",
      "f_med(power_law_exp): 0.026710\n",
      "f_avg(gini): 0.001592\n",
      "f_med(gini): 0.001015\n",
      "f_avg(assortativity): 0.617936\n",
      "f_med(assortativity): 0.083115\n",
      "f_avg(n_component): 0.007296\n",
      "f_med(n_component): 0.006054\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch: 051, overall loss: 1.9793766164\n",
      "Epoch: 052, overall loss: 1.9786688578\n",
      "Epoch: 053, overall loss: 1.9769840581\n",
      "Epoch: 054, overall loss: 1.9714375250\n",
      "Epoch: 055, overall loss: 1.9698000080\n",
      "Epoch: 056, overall loss: 1.9684697795\n",
      "Epoch: 057, overall loss: 1.9691514675\n",
      "Epoch: 058, overall loss: 1.9769601952\n",
      "Epoch: 059, overall loss: 1.9794252213\n",
      "Epoch: 060, overall loss: 1.9794404631\n",
      "Epoch: 061, overall loss: 1.9769568921\n",
      "Epoch: 062, overall loss: 1.9772188915\n",
      "Epoch: 063, overall loss: 1.9710775929\n",
      "Epoch: 064, overall loss: 1.9750358811\n",
      "Epoch: 065, overall loss: 1.9673377832\n",
      "Epoch: 066, overall loss: 1.9632868338\n",
      "Epoch: 067, overall loss: 1.9627040080\n",
      "Epoch: 068, overall loss: 1.9573809429\n",
      "Epoch: 069, overall loss: 1.9548248643\n",
      "Epoch: 070, overall loss: 1.9533690817\n",
      "Epoch: 071, overall loss: 1.9505743316\n",
      "Epoch: 072, overall loss: 1.9553504316\n",
      "Epoch: 073, overall loss: 1.9600304686\n",
      "Epoch: 074, overall loss: 1.9611064476\n",
      "Epoch: 075, overall loss: 1.9613219219\n",
      "Epoch: 076, overall loss: 1.9625925981\n",
      "Epoch: 077, overall loss: 1.9627933859\n",
      "Epoch: 078, overall loss: 1.9624889667\n",
      "Epoch: 079, overall loss: 1.9616442952\n",
      "Epoch: 080, overall loss: 1.9579524166\n",
      "Epoch: 081, overall loss: 1.9521581386\n",
      "Epoch: 082, overall loss: 1.9497965327\n",
      "Epoch: 083, overall loss: 1.9473726216\n",
      "Epoch: 084, overall loss: 1.9445120153\n",
      "Epoch: 085, overall loss: 1.9451921969\n",
      "Epoch: 086, overall loss: 1.9459584377\n",
      "Epoch: 087, overall loss: 1.9469488613\n",
      "Epoch: 088, overall loss: 1.9501821870\n",
      "Epoch: 089, overall loss: 1.9499077758\n",
      "Epoch: 090, overall loss: 1.9484782792\n",
      "Epoch: 091, overall loss: 1.9478445539\n",
      "Epoch: 092, overall loss: 1.9496888431\n",
      "Epoch: 093, overall loss: 1.9472653421\n",
      "Epoch: 094, overall loss: 1.9447947662\n",
      "Epoch: 095, overall loss: 1.9458156800\n",
      "Epoch: 096, overall loss: 1.9486360542\n",
      "Epoch: 097, overall loss: 1.9455912305\n",
      "Epoch: 098, overall loss: 1.9469907340\n",
      "Epoch: 099, overall loss: 1.9453828804\n",
      "Epoch: 100, overall loss: 1.9437223522\n",
      "Epoch: 100, Edge Overlap: 0.942091\n",
      "\n",
      "\n",
      "================================================================================\n",
      "f_avg(d_max): 0.134768\n",
      "f_med(d_max): 0.181818\n",
      "f_avg(d_mean): 0.051948\n",
      "f_med(d_mean): 0.043568\n",
      "f_avg(LCC): 0.064166\n",
      "f_med(LCC): 0.049563\n",
      "f_avg(wedge_count): 0.131613\n",
      "f_med(wedge_count): 0.091085\n",
      "f_avg(claw_count): 0.228960\n",
      "f_med(claw_count): 0.202779\n",
      "f_avg(power_law_exp): 0.033743\n",
      "f_med(power_law_exp): 0.019853\n",
      "f_avg(gini): 0.001458\n",
      "f_med(gini): 0.001050\n",
      "f_avg(assortativity): 0.933375\n",
      "f_med(assortativity): 0.144434\n",
      "f_avg(n_component): 0.006499\n",
      "f_med(n_component): 0.006038\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch: 101, overall loss: 1.9398686929\n",
      "Epoch: 102, overall loss: 1.9378063659\n",
      "Epoch: 103, overall loss: 1.9391515227\n",
      "Epoch: 104, overall loss: 1.9377263827\n",
      "Epoch: 105, overall loss: 1.9382979169\n",
      "Epoch: 106, overall loss: 1.9407110007\n",
      "Epoch: 107, overall loss: 1.9444753513\n",
      "Epoch: 108, overall loss: 1.9461623317\n",
      "Epoch: 109, overall loss: 1.9466247892\n",
      "Epoch: 110, overall loss: 1.9439595335\n",
      "Epoch: 111, overall loss: 1.9421670177\n",
      "Epoch: 112, overall loss: 1.9398395436\n",
      "Epoch: 113, overall loss: 1.9376065676\n",
      "Epoch: 114, overall loss: 1.9353986880\n",
      "Epoch: 115, overall loss: 1.9317403219\n",
      "Epoch: 116, overall loss: 1.9303340707\n",
      "Epoch: 117, overall loss: 1.9305886729\n",
      "Epoch: 118, overall loss: 1.9304894473\n",
      "Epoch: 119, overall loss: 1.9312883668\n",
      "Epoch: 120, overall loss: 1.9326340967\n",
      "Epoch: 121, overall loss: 1.9372351089\n",
      "Epoch: 122, overall loss: 1.9400718324\n",
      "Epoch: 123, overall loss: 1.9387533404\n",
      "Epoch: 124, overall loss: 1.9360777036\n",
      "Epoch: 125, overall loss: 1.9328704323\n",
      "Epoch: 126, overall loss: 1.9301216944\n",
      "Epoch: 127, overall loss: 1.9287140244\n",
      "Epoch: 128, overall loss: 1.9254577156\n",
      "Epoch: 129, overall loss: 1.9209968841\n",
      "Epoch: 130, overall loss: 1.9210278906\n",
      "Epoch: 131, overall loss: 1.9204519240\n",
      "Epoch: 132, overall loss: 1.9200627822\n",
      "Epoch: 133, overall loss: 1.9208002481\n",
      "Epoch: 134, overall loss: 1.9261371692\n",
      "Epoch: 135, overall loss: 1.9274559480\n",
      "Epoch: 136, overall loss: 1.9288466912\n",
      "Epoch: 137, overall loss: 1.9319773910\n",
      "Epoch: 138, overall loss: 1.9338491368\n",
      "Epoch: 139, overall loss: 1.9342559638\n",
      "Epoch: 140, overall loss: 1.9307576894\n",
      "Epoch: 141, overall loss: 1.9284604064\n",
      "Epoch: 142, overall loss: 1.9268510008\n",
      "Epoch: 143, overall loss: 1.9273165219\n",
      "Epoch: 144, overall loss: 1.9234422942\n",
      "Epoch: 145, overall loss: 1.9245279262\n",
      "Epoch: 146, overall loss: 1.9209248235\n",
      "Epoch: 147, overall loss: 1.9180613883\n",
      "Epoch: 148, overall loss: 1.9179768170\n",
      "Epoch: 149, overall loss: 1.9151397914\n",
      "Epoch: 150, overall loss: 1.9138810495\n",
      "Epoch: 150, Edge Overlap: 0.966128\n",
      "\n",
      "\n",
      "================================================================================\n",
      "f_avg(d_max): 0.077458\n",
      "f_med(d_max): 0.076923\n",
      "f_avg(d_mean): 0.036590\n",
      "f_med(d_mean): 0.033784\n",
      "f_avg(LCC): 0.034922\n",
      "f_med(LCC): 0.029570\n",
      "f_avg(wedge_count): 0.082085\n",
      "f_med(wedge_count): 0.087838\n",
      "f_avg(claw_count): 0.138408\n",
      "f_med(claw_count): 0.113884\n",
      "f_avg(power_law_exp): 0.021312\n",
      "f_med(power_law_exp): 0.016801\n",
      "f_avg(gini): 0.000827\n",
      "f_med(gini): 0.000210\n",
      "f_avg(assortativity): 0.465404\n",
      "f_med(assortativity): 0.083641\n",
      "f_avg(n_component): 0.004463\n",
      "f_med(n_component): 0.004602\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch: 151, overall loss: 1.9150481850\n",
      "Epoch: 152, overall loss: 1.9157377798\n",
      "Epoch: 153, overall loss: 1.9177077204\n",
      "Epoch: 154, overall loss: 1.9194924312\n",
      "Epoch: 155, overall loss: 1.9229049591\n",
      "Epoch: 156, overall loss: 1.9233049101\n",
      "Epoch: 157, overall loss: 1.9229572086\n",
      "Epoch: 158, overall loss: 1.9229350516\n",
      "Epoch: 159, overall loss: 1.9228200653\n",
      "Epoch: 160, overall loss: 1.9201503645\n",
      "Epoch: 161, overall loss: 1.9193229841\n",
      "Epoch: 162, overall loss: 1.9176101813\n",
      "Epoch: 163, overall loss: 1.9151525506\n",
      "Epoch: 164, overall loss: 1.9166750444\n",
      "Epoch: 165, overall loss: 1.9147568942\n",
      "Epoch: 166, overall loss: 1.9150161142\n",
      "Epoch: 167, overall loss: 1.9130480348\n",
      "Epoch: 168, overall loss: 1.9142529624\n",
      "Epoch: 169, overall loss: 1.9147091464\n",
      "Epoch: 170, overall loss: 1.9151995178\n",
      "Epoch: 171, overall loss: 1.9149773965\n",
      "Epoch: 172, overall loss: 1.9143496842\n",
      "Epoch: 173, overall loss: 1.9145193378\n",
      "Epoch: 174, overall loss: 1.9154736274\n",
      "Epoch: 175, overall loss: 1.9167502544\n",
      "Epoch: 176, overall loss: 1.9174871772\n",
      "Epoch: 177, overall loss: 1.9161956093\n",
      "Epoch: 178, overall loss: 1.9178679355\n",
      "Epoch: 179, overall loss: 1.9177246350\n",
      "Epoch: 180, overall loss: 1.9181237816\n",
      "Epoch: 181, overall loss: 1.9159363985\n",
      "Epoch: 182, overall loss: 1.9139820023\n",
      "Epoch: 183, overall loss: 1.9129754845\n",
      "Epoch: 184, overall loss: 1.9098526993\n",
      "Epoch: 185, overall loss: 1.9080849656\n",
      "Epoch: 186, overall loss: 1.9076421292\n",
      "Epoch: 187, overall loss: 1.9054743761\n",
      "Epoch: 188, overall loss: 1.9056460139\n",
      "Epoch: 189, overall loss: 1.9039253262\n",
      "Epoch: 190, overall loss: 1.9044026820\n",
      "Epoch: 191, overall loss: 1.9055607797\n",
      "Epoch: 192, overall loss: 1.9062130729\n",
      "Epoch: 193, overall loss: 1.9059109315\n",
      "Epoch: 194, overall loss: 1.9070325490\n",
      "Epoch: 195, overall loss: 1.9078340142\n",
      "Epoch: 196, overall loss: 1.9079506292\n",
      "Epoch: 197, overall loss: 1.9093398931\n",
      "Epoch: 198, overall loss: 1.9095225295\n",
      "Epoch: 199, overall loss: 1.9101489977\n",
      "Epoch: 200, overall loss: 1.9104153008\n",
      "Epoch: 200, Edge Overlap: 0.971470\n",
      "\n",
      "\n",
      "================================================================================\n",
      "f_avg(d_max): 0.057705\n",
      "f_med(d_max): 0.083916\n",
      "f_avg(d_mean): 0.028964\n",
      "f_med(d_mean): 0.027791\n",
      "f_avg(LCC): 0.029523\n",
      "f_med(LCC): 0.033149\n",
      "f_avg(wedge_count): 0.072459\n",
      "f_med(wedge_count): 0.068798\n",
      "f_avg(claw_count): 0.122763\n",
      "f_med(claw_count): 0.091119\n",
      "f_avg(power_law_exp): 0.014869\n",
      "f_med(power_law_exp): 0.013972\n",
      "f_avg(gini): 0.000731\n",
      "f_med(gini): 0.000269\n",
      "f_avg(assortativity): 0.482725\n",
      "f_med(assortativity): 0.070758\n",
      "f_avg(n_component): 0.003860\n",
      "f_med(n_component): 0.004413\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch: 201, overall loss: 1.9098747611\n",
      "Epoch: 202, overall loss: 1.9091984988\n",
      "Epoch: 203, overall loss: 1.9078323089\n",
      "Epoch: 204, overall loss: 1.9066251343\n",
      "Epoch: 205, overall loss: 1.9051516669\n",
      "Epoch: 206, overall loss: 1.9037944316\n",
      "Epoch: 207, overall loss: 1.9030095458\n",
      "Epoch: 208, overall loss: 1.9034400114\n",
      "Epoch: 209, overall loss: 1.9025748485\n",
      "Epoch: 210, overall loss: 1.9016903708\n",
      "Epoch: 211, overall loss: 1.9028196584\n",
      "Epoch: 212, overall loss: 1.9025340333\n",
      "Epoch: 213, overall loss: 1.9039284299\n",
      "Epoch: 214, overall loss: 1.9045588732\n",
      "Epoch: 215, overall loss: 1.9045251499\n",
      "Epoch: 216, overall loss: 1.9047975867\n",
      "Epoch: 217, overall loss: 1.9047899190\n",
      "Epoch: 218, overall loss: 1.9054549991\n",
      "Epoch: 219, overall loss: 1.9034333320\n",
      "Epoch: 220, overall loss: 1.9036202162\n",
      "Epoch: 221, overall loss: 1.9036442635\n",
      "Epoch: 222, overall loss: 1.9026431326\n",
      "Epoch: 223, overall loss: 1.9010288064\n",
      "Epoch: 224, overall loss: 1.9014154084\n",
      "Epoch: 225, overall loss: 1.9001410100\n",
      "Epoch: 226, overall loss: 1.8995241204\n",
      "Epoch: 227, overall loss: 1.8987349089\n",
      "Epoch: 228, overall loss: 1.8992618944\n",
      "Epoch: 229, overall loss: 1.8985987875\n",
      "Epoch: 230, overall loss: 1.8973098080\n",
      "Epoch: 231, overall loss: 1.8975884526\n",
      "Epoch: 232, overall loss: 1.8977365807\n",
      "Epoch: 233, overall loss: 1.8974469731\n",
      "Epoch: 234, overall loss: 1.8971132517\n",
      "Epoch: 235, overall loss: 1.8966670552\n",
      "Epoch: 236, overall loss: 1.8974856412\n",
      "Epoch: 237, overall loss: 1.8962513359\n",
      "Epoch: 238, overall loss: 1.8963845031\n",
      "Epoch: 239, overall loss: 1.8957939211\n",
      "Epoch: 240, overall loss: 1.8953518514\n",
      "Epoch: 241, overall loss: 1.8954149623\n",
      "Epoch: 242, overall loss: 1.8947458086\n",
      "Epoch: 243, overall loss: 1.8948474545\n",
      "Epoch: 244, overall loss: 1.8951733758\n",
      "Epoch: 245, overall loss: 1.8954437087\n",
      "Epoch: 246, overall loss: 1.8947731407\n",
      "Epoch: 247, overall loss: 1.8940711084\n",
      "Epoch: 248, overall loss: 1.8945387387\n",
      "Epoch: 249, overall loss: 1.8941631121\n",
      "Epoch: 250, overall loss: 1.8937900327\n",
      "Epoch: 250, Edge Overlap: 0.983125\n",
      "\n",
      "\n",
      "================================================================================\n",
      "f_avg(d_max): 0.079433\n",
      "f_med(d_max): 0.139610\n",
      "f_avg(d_mean): 0.017860\n",
      "f_med(d_mean): 0.017457\n",
      "f_avg(LCC): 0.032586\n",
      "f_med(LCC): 0.022320\n",
      "f_avg(wedge_count): 0.051266\n",
      "f_med(wedge_count): 0.045087\n",
      "f_avg(claw_count): 0.112799\n",
      "f_med(claw_count): 0.085934\n",
      "f_avg(power_law_exp): 0.007082\n",
      "f_med(power_law_exp): 0.005844\n",
      "f_avg(gini): 0.000583\n",
      "f_med(gini): 0.000403\n",
      "f_avg(assortativity): 0.274216\n",
      "f_med(assortativity): 0.065597\n",
      "f_avg(n_component): 0.002615\n",
      "f_med(n_component): 0.002473\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch: 251, overall loss: 1.8942698415\n",
      "Epoch: 252, overall loss: 1.8935131286\n",
      "Epoch: 253, overall loss: 1.8940491316\n",
      "Epoch: 254, overall loss: 1.8937169724\n",
      "Epoch: 255, overall loss: 1.8940070651\n",
      "Epoch: 256, overall loss: 1.8944684486\n",
      "Epoch: 257, overall loss: 1.8943653278\n",
      "Epoch: 258, overall loss: 1.8947867150\n",
      "Epoch: 259, overall loss: 1.8936791969\n",
      "Epoch: 260, overall loss: 1.8935236349\n",
      "Epoch: 261, overall loss: 1.8939540600\n",
      "Epoch: 262, overall loss: 1.8937867210\n",
      "Epoch: 263, overall loss: 1.8937770484\n",
      "Epoch: 264, overall loss: 1.8941850205\n",
      "Epoch: 265, overall loss: 1.8936635751\n",
      "Epoch: 266, overall loss: 1.8938379123\n",
      "Epoch: 267, overall loss: 1.8937467139\n",
      "Epoch: 268, overall loss: 1.8927541153\n",
      "Epoch: 269, overall loss: 1.8917218104\n",
      "Epoch: 270, overall loss: 1.8912552182\n",
      "Epoch: 271, overall loss: 1.8913820175\n",
      "Epoch: 272, overall loss: 1.8909748392\n",
      "Epoch: 273, overall loss: 1.8903114830\n",
      "Epoch: 274, overall loss: 1.8905938684\n",
      "Epoch: 275, overall loss: 1.8905903189\n",
      "Epoch: 276, overall loss: 1.8905841276\n",
      "Epoch: 277, overall loss: 1.8904643771\n",
      "Epoch: 278, overall loss: 1.8905231590\n",
      "Epoch: 279, overall loss: 1.8903996424\n",
      "Epoch: 280, overall loss: 1.8905884177\n",
      "Epoch: 281, overall loss: 1.8893999383\n",
      "Epoch: 282, overall loss: 1.8895233485\n",
      "Epoch: 283, overall loss: 1.8896810079\n",
      "Epoch: 284, overall loss: 1.8895167275\n",
      "Epoch: 285, overall loss: 1.8892505699\n",
      "Epoch: 286, overall loss: 1.8894980638\n",
      "Epoch: 287, overall loss: 1.8888834945\n",
      "Epoch: 288, overall loss: 1.8889683685\n",
      "Epoch: 289, overall loss: 1.8887251536\n",
      "Epoch: 290, overall loss: 1.8893296234\n",
      "Epoch: 291, overall loss: 1.8882879837\n",
      "Epoch: 292, overall loss: 1.8879741029\n",
      "Epoch: 293, overall loss: 1.8874776697\n",
      "Epoch: 294, overall loss: 1.8869921045\n",
      "Epoch: 295, overall loss: 1.8866277574\n",
      "Epoch: 296, overall loss: 1.8857892239\n",
      "Epoch: 297, overall loss: 1.8852905251\n",
      "Epoch: 298, overall loss: 1.8850654369\n",
      "Epoch: 299, overall loss: 1.8844441398\n",
      "Epoch: 300, overall loss: 1.8841788962\n",
      "Epoch: 300, Edge Overlap: 0.991138\n",
      "\n",
      "\n",
      "================================================================================\n",
      "f_avg(d_max): 0.026529\n",
      "f_med(d_max): 0.076812\n",
      "f_avg(d_mean): 0.009390\n",
      "f_med(d_mean): 0.008368\n",
      "f_avg(LCC): 0.011052\n",
      "f_med(LCC): 0.010364\n",
      "f_avg(wedge_count): 0.025465\n",
      "f_med(wedge_count): 0.029762\n",
      "f_avg(claw_count): 0.046798\n",
      "f_med(claw_count): 0.058480\n",
      "f_avg(power_law_exp): 0.003392\n",
      "f_med(power_law_exp): 0.002705\n",
      "f_avg(gini): 0.000269\n",
      "f_med(gini): 0.000155\n",
      "f_avg(assortativity): 0.038952\n",
      "f_med(assortativity): 0.029614\n",
      "f_avg(n_component): 0.001553\n",
      "f_med(n_component): 0.001642\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch: 301, overall loss: 1.8843125487\n",
      "Epoch: 302, overall loss: 1.8842172002\n",
      "Epoch: 303, overall loss: 1.8841102445\n",
      "Epoch: 304, overall loss: 1.8840836058\n",
      "Epoch: 305, overall loss: 1.8839769053\n",
      "Epoch: 306, overall loss: 1.8840667836\n",
      "Epoch: 307, overall loss: 1.8838134061\n",
      "Epoch: 308, overall loss: 1.8835864038\n",
      "Epoch: 309, overall loss: 1.8838328863\n",
      "Epoch: 310, overall loss: 1.8838011530\n",
      "Epoch: 311, overall loss: 1.8838892849\n",
      "Epoch: 312, overall loss: 1.8839538116\n",
      "Epoch: 313, overall loss: 1.8840649481\n",
      "Epoch: 314, overall loss: 1.8838949006\n",
      "Epoch: 315, overall loss: 1.8839326223\n",
      "Epoch: 316, overall loss: 1.8837535433\n",
      "Epoch: 317, overall loss: 1.8835678634\n",
      "Epoch: 318, overall loss: 1.8836515029\n",
      "Epoch: 319, overall loss: 1.8834554376\n",
      "Epoch: 320, overall loss: 1.8834224334\n",
      "Epoch: 321, overall loss: 1.8834863132\n",
      "Epoch: 322, overall loss: 1.8834816723\n",
      "Epoch: 323, overall loss: 1.8832824409\n",
      "Epoch: 324, overall loss: 1.8831994599\n",
      "Epoch: 325, overall loss: 1.8832395354\n",
      "Epoch: 326, overall loss: 1.8829522464\n",
      "Epoch: 327, overall loss: 1.8827760556\n",
      "Epoch: 328, overall loss: 1.8826007616\n",
      "Epoch: 329, overall loss: 1.8824258484\n",
      "Epoch: 330, overall loss: 1.8821164427\n",
      "Epoch: 331, overall loss: 1.8820916050\n",
      "Epoch: 332, overall loss: 1.8819884250\n",
      "Epoch: 333, overall loss: 1.8818615925\n",
      "Epoch: 334, overall loss: 1.8816676251\n",
      "Epoch: 335, overall loss: 1.8815176565\n",
      "Epoch: 336, overall loss: 1.8813263154\n",
      "Epoch: 337, overall loss: 1.8812566815\n",
      "Epoch: 338, overall loss: 1.8811867776\n",
      "Epoch: 339, overall loss: 1.8811362181\n",
      "Epoch: 340, overall loss: 1.8809907568\n",
      "Epoch: 341, overall loss: 1.8809683987\n",
      "Epoch: 342, overall loss: 1.8808909868\n",
      "Epoch: 343, overall loss: 1.8808347246\n",
      "Epoch: 344, overall loss: 1.8807519158\n",
      "Epoch: 345, overall loss: 1.8806965636\n",
      "Epoch: 346, overall loss: 1.8806290328\n",
      "Epoch: 347, overall loss: 1.8805637694\n",
      "Epoch: 348, overall loss: 1.8805245056\n",
      "Epoch: 349, overall loss: 1.8805339443\n",
      "Epoch: 350, overall loss: 1.8804195257\n",
      "Epoch: 350, Edge Overlap: 0.994294\n",
      "\n",
      "\n",
      "================================================================================\n",
      "f_avg(d_max): 0.022426\n",
      "f_med(d_max): 0.069712\n",
      "f_avg(d_mean): 0.006773\n",
      "f_med(d_mean): 0.006024\n",
      "f_avg(LCC): 0.013954\n",
      "f_med(LCC): 0.011494\n",
      "f_avg(wedge_count): 0.018382\n",
      "f_med(wedge_count): 0.019440\n",
      "f_avg(claw_count): 0.035566\n",
      "f_med(claw_count): 0.033138\n",
      "f_avg(power_law_exp): 0.002328\n",
      "f_med(power_law_exp): 0.002683\n",
      "f_avg(gini): 0.000205\n",
      "f_med(gini): 0.000081\n",
      "f_avg(assortativity): 0.123147\n",
      "f_med(assortativity): 0.024950\n",
      "f_avg(n_component): 0.001392\n",
      "f_med(n_component): 0.001754\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch: 351, overall loss: 1.8804018389\n",
      "Epoch: 352, overall loss: 1.8803542436\n",
      "Epoch: 353, overall loss: 1.8803583381\n",
      "Epoch: 354, overall loss: 1.8802895834\n",
      "Epoch: 355, overall loss: 1.8802698424\n",
      "Epoch: 356, overall loss: 1.8802057886\n",
      "Epoch: 357, overall loss: 1.8801809144\n",
      "Epoch: 358, overall loss: 1.8801539965\n",
      "Epoch: 359, overall loss: 1.8801183960\n",
      "Epoch: 360, overall loss: 1.8800865168\n",
      "Epoch: 361, overall loss: 1.8800690632\n",
      "Epoch: 362, overall loss: 1.8800487480\n",
      "Epoch: 363, overall loss: 1.8800415256\n",
      "Epoch: 364, overall loss: 1.8800387019\n",
      "Epoch: 365, overall loss: 1.8800339679\n",
      "Epoch: 366, overall loss: 1.8800250890\n",
      "Epoch: 367, overall loss: 1.8800150868\n",
      "Epoch: 368, overall loss: 1.8799943219\n",
      "Epoch: 369, overall loss: 1.8799979726\n",
      "Epoch: 370, overall loss: 1.8799908834\n",
      "Epoch: 371, overall loss: 1.8799828108\n",
      "Epoch: 372, overall loss: 1.8799617483\n",
      "Epoch: 373, overall loss: 1.8799623068\n",
      "Epoch: 374, overall loss: 1.8799357956\n",
      "Epoch: 375, overall loss: 1.8799205450\n",
      "Epoch: 376, overall loss: 1.8799017381\n",
      "Epoch: 377, overall loss: 1.8798806445\n",
      "Epoch: 378, overall loss: 1.8798713220\n",
      "Epoch: 379, overall loss: 1.8798551120\n",
      "Epoch: 380, overall loss: 1.8798450995\n",
      "Epoch: 381, overall loss: 1.8798299967\n",
      "Epoch: 382, overall loss: 1.8798207447\n",
      "Epoch: 383, overall loss: 1.8798110174\n",
      "Epoch: 384, overall loss: 1.8798035366\n",
      "Epoch: 385, overall loss: 1.8797974056\n",
      "Epoch: 386, overall loss: 1.8797904449\n",
      "Epoch: 387, overall loss: 1.8797846522\n",
      "Epoch: 388, overall loss: 1.8797794488\n",
      "Epoch: 389, overall loss: 1.8797741868\n",
      "Epoch: 390, overall loss: 1.8797691513\n",
      "Epoch: 391, overall loss: 1.8797648298\n",
      "Epoch: 392, overall loss: 1.8797604088\n",
      "Epoch: 393, overall loss: 1.8797562276\n",
      "Epoch: 394, overall loss: 1.8797521367\n",
      "Epoch: 395, overall loss: 1.8797483979\n",
      "Epoch: 396, overall loss: 1.8797445814\n",
      "Epoch: 397, overall loss: 1.8797409604\n",
      "Epoch: 398, overall loss: 1.8797372284\n",
      "Epoch: 399, overall loss: 1.8797337157\n",
      "Epoch: 400, overall loss: 1.8797303815\n",
      "Epoch: 400, Edge Overlap: 0.998300\n",
      "\n",
      "\n",
      "================================================================================\n",
      "f_avg(d_max): 0.006061\n",
      "f_med(d_max): 0.090909\n",
      "f_avg(d_mean): 0.003330\n",
      "f_med(d_mean): 0.003199\n",
      "f_avg(LCC): 0.004229\n",
      "f_med(LCC): 0.005525\n",
      "f_avg(wedge_count): 0.011773\n",
      "f_med(wedge_count): 0.012784\n",
      "f_avg(claw_count): 0.026019\n",
      "f_med(claw_count): 0.023417\n",
      "f_avg(power_law_exp): 0.002116\n",
      "f_med(power_law_exp): 0.001024\n",
      "f_avg(gini): 0.000167\n",
      "f_med(gini): 0.000136\n",
      "f_avg(assortativity): 0.132328\n",
      "f_med(assortativity): 0.024423\n",
      "f_avg(n_component): 0.000575\n",
      "f_med(n_component): 0.000890\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch: 401, overall loss: 1.8797270658\n",
      "Epoch: 402, overall loss: 1.8797238874\n",
      "Epoch: 403, overall loss: 1.8797205749\n",
      "Epoch: 404, overall loss: 1.8797175123\n",
      "Epoch: 405, overall loss: 1.8797145369\n",
      "Epoch: 406, overall loss: 1.8797115994\n",
      "Epoch: 407, overall loss: 1.8797087098\n",
      "Epoch: 408, overall loss: 1.8797058059\n",
      "Epoch: 409, overall loss: 1.8797032649\n",
      "Epoch: 410, overall loss: 1.8797005183\n",
      "Epoch: 411, overall loss: 1.8796978373\n",
      "Epoch: 412, overall loss: 1.8796953232\n",
      "Epoch: 413, overall loss: 1.8796928425\n",
      "Epoch: 414, overall loss: 1.8796904281\n",
      "Epoch: 415, overall loss: 1.8796880232\n",
      "Epoch: 416, overall loss: 1.8796857435\n",
      "Epoch: 417, overall loss: 1.8796835300\n",
      "Epoch: 418, overall loss: 1.8796813935\n",
      "Epoch: 419, overall loss: 1.8796792773\n",
      "Epoch: 420, overall loss: 1.8796771608\n",
      "Epoch: 421, overall loss: 1.8796750276\n",
      "Epoch: 422, overall loss: 1.8796730270\n",
      "Epoch: 423, overall loss: 1.8796711590\n",
      "Epoch: 424, overall loss: 1.8796692088\n",
      "Epoch: 425, overall loss: 1.8796674290\n",
      "Epoch: 426, overall loss: 1.8796656059\n",
      "Epoch: 427, overall loss: 1.8796639929\n",
      "Epoch: 428, overall loss: 1.8796622955\n",
      "Epoch: 429, overall loss: 1.8796605427\n",
      "Epoch: 430, overall loss: 1.8796589017\n",
      "Epoch: 431, overall loss: 1.8796573405\n",
      "Epoch: 432, overall loss: 1.8796558384\n",
      "Epoch: 433, overall loss: 1.8796543775\n",
      "Epoch: 434, overall loss: 1.8796529271\n",
      "Epoch: 435, overall loss: 1.8796515504\n",
      "Epoch: 436, overall loss: 1.8796501406\n",
      "Epoch: 437, overall loss: 1.8796488756\n",
      "Epoch: 438, overall loss: 1.8796476974\n",
      "Epoch: 439, overall loss: 1.8796463945\n",
      "Epoch: 440, overall loss: 1.8796451341\n",
      "Epoch: 441, overall loss: 1.8796440298\n",
      "Epoch: 442, overall loss: 1.8796428925\n",
      "Epoch: 443, overall loss: 1.8796417623\n",
      "Epoch: 444, overall loss: 1.8796407027\n",
      "Epoch: 445, overall loss: 1.8796396502\n",
      "Epoch: 446, overall loss: 1.8796386816\n",
      "Epoch: 447, overall loss: 1.8796377584\n",
      "Epoch: 448, overall loss: 1.8796367728\n",
      "Epoch: 449, overall loss: 1.8796358946\n",
      "Epoch: 450, overall loss: 1.8796350016\n",
      "Epoch: 450, Edge Overlap: 0.998665\n",
      "\n",
      "\n",
      "================================================================================\n",
      "f_avg(d_max): 0.007949\n",
      "f_med(d_max): 0.043478\n",
      "f_avg(d_mean): 0.003511\n",
      "f_med(d_mean): 0.004286\n",
      "f_avg(LCC): 0.005947\n",
      "f_med(LCC): 0.007969\n",
      "f_avg(wedge_count): 0.008248\n",
      "f_med(wedge_count): 0.008187\n",
      "f_avg(claw_count): 0.014973\n",
      "f_med(claw_count): 0.011599\n",
      "f_avg(power_law_exp): 0.001629\n",
      "f_med(power_law_exp): 0.001780\n",
      "f_avg(gini): 0.000132\n",
      "f_med(gini): 0.000154\n",
      "f_avg(assortativity): 0.040588\n",
      "f_med(assortativity): 0.019944\n",
      "f_avg(n_component): 0.001130\n",
      "f_med(n_component): 0.001858\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch: 451, overall loss: 1.8796341666\n",
      "Epoch: 452, overall loss: 1.8796333913\n",
      "Epoch: 453, overall loss: 1.8796325946\n",
      "Epoch: 454, overall loss: 1.8796318470\n",
      "Epoch: 455, overall loss: 1.8796311488\n",
      "Epoch: 456, overall loss: 1.8796304123\n",
      "Epoch: 457, overall loss: 1.8796297325\n",
      "Epoch: 458, overall loss: 1.8796291750\n",
      "Epoch: 459, overall loss: 1.8796284186\n",
      "Epoch: 460, overall loss: 1.8796279126\n",
      "Epoch: 461, overall loss: 1.8796273753\n",
      "Epoch: 462, overall loss: 1.8796267319\n",
      "Epoch: 463, overall loss: 1.8796262864\n",
      "Epoch: 464, overall loss: 1.8796257095\n",
      "Epoch: 465, overall loss: 1.8796253618\n",
      "Epoch: 466, overall loss: 1.8796248368\n",
      "Epoch: 467, overall loss: 1.8796244898\n",
      "Epoch: 468, overall loss: 1.8796240491\n",
      "Epoch: 469, overall loss: 1.8796236914\n",
      "Epoch: 470, overall loss: 1.8796233198\n",
      "Epoch: 471, overall loss: 1.8796229102\n",
      "Epoch: 472, overall loss: 1.8796226130\n",
      "Epoch: 473, overall loss: 1.8796223677\n",
      "Epoch: 474, overall loss: 1.8796220398\n",
      "Epoch: 475, overall loss: 1.8796218537\n",
      "Epoch: 476, overall loss: 1.8796215301\n",
      "Epoch: 477, overall loss: 1.8796213644\n",
      "Epoch: 478, overall loss: 1.8796211008\n",
      "Epoch: 479, overall loss: 1.8796209441\n",
      "Epoch: 480, overall loss: 1.8796207168\n",
      "Epoch: 481, overall loss: 1.8796205473\n",
      "Epoch: 482, overall loss: 1.8796204365\n",
      "Epoch: 483, overall loss: 1.8796202991\n",
      "Epoch: 484, overall loss: 1.8796201289\n",
      "Epoch: 485, overall loss: 1.8796200828\n",
      "Epoch: 486, overall loss: 1.8796199376\n",
      "Epoch: 487, overall loss: 1.8796198088\n",
      "Epoch: 488, overall loss: 1.8796197123\n",
      "Epoch: 489, overall loss: 1.8796196894\n",
      "Epoch: 490, overall loss: 1.8796196134\n",
      "Epoch: 491, overall loss: 1.8796195403\n",
      "Epoch: 492, overall loss: 1.8796195472\n",
      "Epoch: 493, overall loss: 1.8796194438\n",
      "Epoch: 494, overall loss: 1.8796194247\n",
      "Epoch: 495, overall loss: 1.8796194455\n",
      "Epoch: 496, overall loss: 1.8796194815\n",
      "Epoch: 497, overall loss: 1.8796193225\n",
      "Epoch: 498, overall loss: 1.8796193262\n",
      "Epoch: 499, overall loss: 1.8796193835\n",
      "Epoch: 500, overall loss: 1.8796193537\n",
      "Epoch: 500, Edge Overlap: 0.999272\n",
      "\n",
      "\n",
      "================================================================================\n",
      "f_avg(d_max): 0.006187\n",
      "f_med(d_max): 0.046402\n",
      "f_avg(d_mean): 0.005702\n",
      "f_med(d_mean): 0.004592\n",
      "f_avg(LCC): 0.006536\n",
      "f_med(LCC): 0.009346\n",
      "f_avg(wedge_count): 0.010457\n",
      "f_med(wedge_count): 0.009672\n",
      "f_avg(claw_count): 0.014964\n",
      "f_med(claw_count): 0.008772\n",
      "f_avg(power_law_exp): 0.003749\n",
      "f_med(power_law_exp): 0.002111\n",
      "f_avg(gini): 0.000124\n",
      "f_med(gini): 0.000098\n",
      "f_avg(assortativity): 0.027490\n",
      "f_med(assortativity): 0.016124\n",
      "f_avg(n_component): 0.000607\n",
      "f_med(n_component): 0.001101\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Finished training.\n",
      "Best f_avg(d_max): 0.006061\n",
      "Best f_med(d_max): 0.043478\n",
      "Best f_avg(d_mean): 0.003330\n",
      "Best f_med(d_mean): 0.003199\n",
      "Best f_avg(LCC): 0.004229\n",
      "Best f_med(LCC): 0.005525\n",
      "Best f_avg(wedge_count): 0.008248\n",
      "Best f_med(wedge_count): 0.008187\n",
      "Best f_avg(claw_count): 0.014964\n",
      "Best f_med(claw_count): 0.008772\n",
      "Best f_avg(power_law_exp): 0.001629\n",
      "Best f_med(power_law_exp): 0.001024\n",
      "Best f_avg(gini): 0.000124\n",
      "Best f_med(gini): 0.000081\n",
      "Best f_avg(assortativity): 0.027490\n",
      "Best f_med(assortativity): 0.016124\n",
      "Best f_avg(n_component): 0.000575\n",
      "Best f_med(n_component): 0.000890\n"
     ]
    }
   ],
   "source": [
    "label_adj, nids = FromTemporalGraphToSparseAdj()\n",
    "label_mat = label_adj.tocsr()[nids, :]\n",
    "num_nodes = label_adj.shape[1]\n",
    "t = label_adj.shape[0] // num_nodes\n",
    "feat = sp.diags(np.ones(num_nodes * t).astype(np.float32)).tocsr()\n",
    "adj = label_adj.tocsr()\n",
    "best_results = {}\n",
    "sp.save_npz(os.path.join(\"./data/DBLP\", \"adj.npz\"), adj)\n",
    "\n",
    "dgl_g = dgl.load_graphs(os.path.join(\"./data/DBLP\", \"dgl_graph.bin\"))[0][0]\n",
    "dgl_g = dgl.add_self_loop(dgl_g)\n",
    "train_sampler = MultiLayerFullNeighborSampler(num_layers=args.n_layers)\n",
    "train_dataloader = DataLoader(dgl_g.to(args.device),\n",
    "                                    indices=torch.from_numpy(nids).long().to(args.device),\n",
    "                                    graph_sampler=train_sampler,\n",
    "                                    device=args.device,\n",
    "                                    batch_size=args.batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    drop_last=False,\n",
    "                                    num_workers=0)\n",
    "model = ScalableTGAE(in_dim=num_nodes * t,\n",
    "                        hid_dim=int(args.H/args.n_heads),\n",
    "                        n_heads=args.n_heads,\n",
    "                        out_dim=num_nodes).to(args.device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.max_epochs, eta_min=1e-6)\n",
    "\n",
    "best_f_avg = {}\n",
    "best_f_med = {}\n",
    "\n",
    "for epoch in range(args.max_epochs):\n",
    "    num_edges_all = 0\n",
    "    num_loss_all = 0\n",
    "    model.train()\n",
    "    for step, (input_nodes, seeds, blocks) in enumerate(train_dataloader):\n",
    "        batch_inputs, batch_labels = coo_to_csp(feat[input_nodes.cpu(), :].tocoo()).to(args.device), \\\n",
    "                                        coo_to_csp(adj[seeds.cpu(), :].tocoo()).to_dense().to(args.device)\n",
    "        blocks = [block.to(args.device) for block in blocks]\n",
    "        train_batch_logits = model(blocks, batch_inputs)\n",
    "        num_edges = batch_labels.sum() / 2\n",
    "        num_edges_all += num_edges\n",
    "        loss = -0.5 * torch.sum(batch_labels * torch.log_softmax(train_batch_logits, dim=-1)) / num_edges\n",
    "        num_loss_all += loss.cpu().data * num_edges\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (step+1) % 50 == 0:\n",
    "            print(\"Epoch: {:03d}, Step: {:03d}, loss: {:.7f}\".format(epoch+1, step+1, loss.cpu().data))\n",
    "        else:\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"Epoch: {:03d}, Step: {:03d}, loss: {:.7f}\\r\".format(epoch+1, step+1, loss.cpu().data))\n",
    "            sys.stdout.flush()\n",
    "    scheduler.step()\n",
    "    print(\"Epoch: {:03d}, overall loss: {:.7f}\".format(epoch + 1, num_loss_all/num_edges_all))\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        gen_mat = sp.csr_matrix(adj.shape)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, (input_nodes, seeds, blocks) in enumerate(train_dataloader):\n",
    "                test_inputs = coo_to_csp(feat[input_nodes.cpu(), :].tocoo()).to(args.device)\n",
    "                blocks = [block for block in blocks]\n",
    "                test_batch_logits = torch.softmax(model(blocks, test_inputs), dim=-1)\n",
    "                num_edges = adj[seeds.cpu(), :].sum()\n",
    "                gen_mat[seeds.cpu(), :] = edge_from_scores(test_batch_logits.cpu().numpy(), num_edges)\n",
    "                if (step+1) % 20 == 0:\n",
    "                    print(\"Epoch: {:03d}, Generating Step: {:03d}\".format(epoch+1, step+1))\n",
    "                else:\n",
    "                    sys.stdout.flush()\n",
    "                    sys.stdout.write(\"Epoch: {:03d}, Generating Step: {:03d}\\r\".format(epoch+1, step+1))\n",
    "                    sys.stdout.flush()\n",
    "        eo = adj.multiply(gen_mat).sum() / adj.sum()\n",
    "        sp.save_npz(os.path.join(\"./data/DBLP\", \"gen_mat.npz\".format(epoch+1)), gen_mat)\n",
    "        print(\"Epoch: {:03d}, Edge Overlap: {:07f}\".format(epoch + 1, eo))\n",
    "        \n",
    "        print('\\n\\n', '='*80, sep='')\n",
    "        f_avg, f_med= cal_avg_median_stats(adj, gen_mat)\n",
    "        for key in f_avg.keys():\n",
    "            print(\"f_avg({}): {:.6f}\".format(key, f_avg[key]))\n",
    "            print(\"f_med({}): {:.6f}\".format(key, f_med[key]))\n",
    "            if key not in best_f_avg or f_avg[key] < best_f_avg[key]:\n",
    "                best_f_avg[key] = f_avg[key]\n",
    "            if key not in best_f_med or f_med[key] < best_f_med[key]:\n",
    "                best_f_med[key] = f_med[key]\n",
    "        print('='*80, '\\n\\n', sep='')\n",
    "print(\"Finished training.\")\n",
    "for key in best_f_avg.keys():\n",
    "    print(\"Best f_avg({}): {:.6f}\".format(key, best_f_avg[key]))\n",
    "    print(\"Best f_med({}): {:.6f}\".format(key, best_f_med[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
